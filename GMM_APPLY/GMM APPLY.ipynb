{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "598e1286-599d-427f-bff7-a707c25bcea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from scipy.stats import multivariate_normal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee03463-3b64-4eca-a2ac-c5147ba22bb9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Gaussian Mixture Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7a5523-c80a-4e03-bbb6-4e9b398ca709",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### K-Means Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ea0fc11-6d07-4e0f-a32f-083f8551cf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeans_clustering:\n",
    "    Data = []\n",
    "    K = 0 #Number of Clusters\n",
    "    E_mean = 0\n",
    "    \n",
    "    def __init__(self,Data,number_of_clusters):\n",
    "        self.Data = Data\n",
    "        self.K = number_of_clusters\n",
    "\n",
    "        epsilon_constant = 0.1\n",
    "        self.E_mean = [] #np.ones((self.K,np.shape(self.Data[0]))) * (0.01)  # This means that less than when change in means is less than 0.01 the process stops\n",
    "        for i in range(self.K):\n",
    "            self.E_mean.append(np.zeros(np.shape(self.Data[0])))\n",
    "\n",
    "        self.E_mean = np.array(self.E_mean)\n",
    "            \n",
    "            \n",
    "\n",
    "    def cluster_allocator(self,new_means_array):\n",
    "        DATA_COPY = np.copy(self.Data)\n",
    "        cluster_array = []\n",
    "\n",
    "        for i in range(self.K):\n",
    "            cluster_array.append([])\n",
    "        \n",
    "        for i in range(len(DATA_COPY)):\n",
    "            #distance calculation of points from means\n",
    "            distance_vectors = new_means_array - DATA_COPY[i]\n",
    "            distances_array = []\n",
    "\n",
    "            for j in distance_vectors:\n",
    "                #print(j)\n",
    "                j = np.matrix(j)\n",
    "                #distances_array.append(np.sqrt(j@j))\n",
    "                distances_array.append(np.matmul(j,np.transpose(j)))\n",
    "            \n",
    "            distances_array = np.array(distances_array)\n",
    "\n",
    "            min_distance_index = np.argmin(distances_array)\n",
    "            cluster_array[min_distance_index].append(i)\n",
    "\n",
    "        return cluster_array\n",
    "\n",
    "    def centroid_calculator(self,cluster_array):\n",
    "        new_means_array = []\n",
    "\n",
    "        for i in cluster_array:\n",
    "            # i is an array of indices\n",
    "            total_vector = 0\n",
    "            mean_vector = 0\n",
    "            for j in i:\n",
    "                #j are indices from the main data array\n",
    "                total_vector += self.Data[j]\n",
    "\n",
    "            mean_vector = total_vector/len(i)\n",
    "            new_means_array.append(mean_vector)\n",
    "\n",
    "        new_means_array = np.array(new_means_array)\n",
    "        return new_means_array\n",
    "\n",
    "                \n",
    "    def cluster_array_decoder(self,cluster_array):\n",
    "        cluster_array_decoded = []\n",
    "\n",
    "        for i in cluster_array:\n",
    "            #we get an array of indices\n",
    "            array_1 = []\n",
    "            for j in i:\n",
    "                #Now we get indices\n",
    "                array_1.append(self.Data[j])\n",
    "\n",
    "            cluster_array_decoded.append(array_1)\n",
    "\n",
    "        return cluster_array_decoded\n",
    "                \n",
    "    \n",
    "    def main_process_single_iteration(self,iterator_limit):\n",
    "        #print('dsfsdfdsd')\n",
    "        DATA = np.copy(self.Data)\n",
    "        new_means_array = [] #np.zeros((self.K,len(DATA[0])))\n",
    "\n",
    "        #initialising means randomly\n",
    "        for i in range(self.K):\n",
    "            random_number = np.random.randint(0,len(DATA))\n",
    "            new_means_array.append(DATA[random_number])\n",
    "            DATA = np.delete(DATA,random_number,axis=0)\n",
    "\n",
    "        #print(new_means_array)\n",
    "        new_means_array = np.array(new_means_array)\n",
    "        DATA = np.copy(self.Data)\n",
    "\n",
    "        \n",
    "        old_means_array = np.zeros_like(new_means_array) \n",
    "        #Now the loop starts\n",
    "        iterator = 1\n",
    " \n",
    "        while True and iterator <= iterator_limit:\n",
    "            #print(iterator)\n",
    "            #Cluster Allocation Step\n",
    "            cluster_array = self.cluster_allocator(new_means_array)\n",
    "            old_means_array = np.copy(new_means_array)\n",
    "\n",
    "            #new means calculation step\n",
    "            new_means_array = self.centroid_calculator(cluster_array)\n",
    "\n",
    "            #difference between old and new array\n",
    "            mean_difference_array = np.abs(new_means_array - old_means_array)#new_means_array - old_means_array\n",
    "\n",
    "            #checking termination condition\n",
    "            number_of_means_satisfying_termination_criteria = np.count_nonzero(mean_difference_array < self.E_mean)\n",
    "\n",
    "            if number_of_means_satisfying_termination_criteria == self.K:\n",
    "                break\n",
    "\n",
    "            iterator += 1\n",
    "\n",
    "        return new_means_array,cluster_array\n",
    "\n",
    "    def WCSS_calculator(self,means_array,cluster_array):  #Within clusters sum of squares\n",
    "        WCSS = 0\n",
    "\n",
    "        decoded_cluster_array = self.cluster_array_decoder(cluster_array)\n",
    "\n",
    "        for count,i in enumerate(decoded_cluster_array):\n",
    "            for j in i:\n",
    "                difference_vector_from_allocated_mean = np.matrix(j - means_array[count])\n",
    "                WCSS += np.matmul(difference_vector_from_allocated_mean,np.transpose(difference_vector_from_allocated_mean))\n",
    "\n",
    "        return WCSS\n",
    "        \n",
    "    def find_optimal_cluster(self,N,iterator_limit):\n",
    "        \n",
    "        cluster_new = np.array([])\n",
    "        means_new = np.array([])\n",
    "        WCSS_new = 0\n",
    "\n",
    "        means_array_old,cluster_array_old = self.main_process_single_iteration(iterator_limit)\n",
    "        WCSS_old = self.WCSS_calculator(means_array_old,cluster_array_old)\n",
    "        \n",
    "        i=0\n",
    "        while i<N:\n",
    "            #print(WCSS_old)\n",
    "            means_array_new,cluster_array_new = self.main_process_single_iteration(iterator_limit)\n",
    "            WCSS_new = self.WCSS_calculator(means_array_new,cluster_array_new)\n",
    "\n",
    "            if WCSS_new < WCSS_old:\n",
    "                WCSS_old = WCSS_new\n",
    "                means_array_old = means_array_new\n",
    "\n",
    "                #print(cluster_array_new)\n",
    "                cluster_array_old = cluster_array_new\n",
    "\n",
    "            elif WCSS_new >= WCSS_old:\n",
    "                #Do Nothning\n",
    "                pass\n",
    "            \n",
    "            i += 1\n",
    "\n",
    "        return means_array_old,self.cluster_array_decoder(cluster_array_old)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d693d3a9-4db2-4e54-9d42-43272295fa5f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Parameter estimation using Expectation Maximisation Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c2ac04e-c83e-4d52-b17a-6b5d2fb8a42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Expectation_Maximisation_FOR_GMM_Params:\n",
    "    DATA = []\n",
    "    number_of_Gaussians = 0\n",
    "    threshold = 0\n",
    "    theta = []  #parameter vector as per notes\n",
    "    N = 0 #Number of Data points\n",
    "    diag_cov_flag = 0\n",
    "\n",
    "    def __init__(self,Data,number_of_Gaussians,threshold=0.001,diagonal_cov_mat=0):\n",
    "        self.diag_cov_flag = diagonal_cov_mat\n",
    "        self.DATA = Data\n",
    "        self.number_of_Gaussians = number_of_Gaussians\n",
    "        #super().__init__(Data,number_of_Gaussians)\n",
    "        self.theta = []\n",
    "        self.N = np.shape(Data)[0]\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def Intialisation_step(self):   #Do K-Means-clustering\n",
    "        w_array = []\n",
    "        means_array = []\n",
    "        covariance_matrix_array = []\n",
    "        Nq_array = []  #number of points in qth cluster/gaussian\n",
    "        \n",
    "        Q = self.number_of_Gaussians\n",
    "\n",
    "        OBJECT_1 = KMeans_clustering(self.DATA,self.number_of_Gaussians)\n",
    "        KMEANS_means_array,KMEANS_cluster_array = OBJECT_1.find_optimal_cluster(3,15) #3,50\n",
    "\n",
    "        for q in range(len(KMEANS_cluster_array)):\n",
    "            KMEANS_cluster_array[q] = np.array(KMEANS_cluster_array[q])\n",
    "\n",
    "        for q in range(Q):\n",
    "            N_q = np.shape(KMEANS_cluster_array[q])[0]\n",
    "            w_q = N_q/self.N\n",
    "            w_array.append(w_q)\n",
    "            \n",
    "            cluster_mean = KMEANS_means_array[q]\n",
    "            means_array.append(cluster_mean)\n",
    "\n",
    "            #calculating the covariance matrix\n",
    "            n_features = np.shape(self.DATA)[1]\n",
    "            cov_matrix = np.zeros((n_features,n_features))\n",
    "\n",
    "            '''\n",
    "            for i in KMEANS_cluster_array[q]:\n",
    "                difference_from_mean = np.matrix(i - cluster_mean)\n",
    "                cov_matrix += (np.matmul(difference_from_mean.T,difference_from_mean))/N_q\n",
    "            '''\n",
    "##############################################\n",
    "            \n",
    "\n",
    "            if self.diag_cov_flag==1:\n",
    "                shape_cov_mat = np.shape(np.cov(KMEANS_cluster_array[q].T))\n",
    "                toappend = np.multiply(np.cov(KMEANS_cluster_array[q].T),np.eye(shape_cov_mat[0]))\n",
    "                covariance_matrix_array.append(toappend)\n",
    "\n",
    "            else:\n",
    "                covariance_matrix_array.append(np.cov(KMEANS_cluster_array[q].T))\n",
    "                \n",
    "##############################################\n",
    "\n",
    "        theta_old = [w_array,means_array,covariance_matrix_array] #as per notes\n",
    "\n",
    "        return theta_old    #,KMEANS_means_array,KMEANS_cluster_array\n",
    "\n",
    "\n",
    "    def NORMAL_DISTRIBUTION(self,mu,cov_mat,Point):\n",
    "        inv_cov_mat = np.linalg.inv(cov_mat)\n",
    "        difference_from_mean = np.matrix(Point - mu)\n",
    "        d = np.shape(Point)[0]\n",
    "        #print(d)\n",
    "\n",
    "        exponentail_term_power = -(np.matmul(np.matmul(difference_from_mean,inv_cov_mat),difference_from_mean.T))/2\n",
    "\n",
    "        rational_term_denominator = np.sqrt(((2*np.pi)**d) * np.linalg.det(cov_mat))\n",
    "        #print(rational_term_denominator)\n",
    "\n",
    "        Normal_prob = np.exp(exponentail_term_power[0,0])/(rational_term_denominator) # + 1)  #regularisation\n",
    "        return Normal_prob\n",
    "        \n",
    "        \n",
    "    def Expectation_step_E(self,theta_old): #KMEANS_means_array,KMEANS_cluster_array):\n",
    "        responsibility_array = np.zeros((self.number_of_Gaussians,self.N))  #structured as per Cluster array\n",
    "\n",
    "        responsibility_numerator_array = np.zeros_like(responsibility_array) #structured as per Cluster array\n",
    "        responsibility_denominator_array = np.zeros(np.shape(self.DATA)[0])\n",
    "        numerator_sum = 0.0\n",
    "\n",
    "        w_array = theta_old[0]\n",
    "        means_array = theta_old[1]\n",
    "        covariance_matrix_array = theta_old[2]\n",
    "        \n",
    "        \n",
    "        for n,i in enumerate(self.DATA):\n",
    "            for q in range(self.number_of_Gaussians):\n",
    "                responsibility_denominator_array[n] += w_array[q] * multivariate_normal.pdf(i,means_array[q],covariance_matrix_array[q])\n",
    "                responsibility_numerator_array[q][n] = w_array[q] * multivariate_normal.pdf(i,means_array[q],covariance_matrix_array[q])\n",
    "        \n",
    "        for q in range(self.number_of_Gaussians):\n",
    "            for n,i in enumerate(self.DATA):\n",
    "                responsibility_array[q][n] = responsibility_numerator_array[q][n]/responsibility_denominator_array[n]\n",
    "                #print(responsibility_numerator_array[q][n])\n",
    "                #numerator_sum += responsibility_numerator_array[q][n]\n",
    "\n",
    "        #responsibility_array = responsibility_numerator_array/numerator_sum\n",
    "\n",
    "        return responsibility_array\n",
    "\n",
    "        '''\n",
    "        for q in range(self.number_of_Gaussians):\n",
    "            qth_row_in_resp_vector = []\n",
    "            cluster_list = KMEANS_cluster_array[q]\n",
    "            wq = theta_old[0][q]\n",
    "            gaussian_mean = theta_old[1][q] #OR KMEANS_means_array[q]\n",
    "            cov_mat = theta_old[2][q]\n",
    "\n",
    "            for j in cluster_list:\n",
    "                gamma_nq = wq * self.NORMAL_DISTRIBUTION(gaussian_mean,cov_mat,j)\n",
    "                numerator_sum += gamma_nq\n",
    "                qth_row_in_resp_vector.append(gamma_nq)\n",
    "\n",
    "            responsibility_numerator_array.append(qth_row_in_resp_vector)\n",
    "\n",
    "        for i in responsibility_numerator_array:\n",
    "            for j in i:\n",
    "                responsibility_array.append(j/numerator_sum)\n",
    "\n",
    "        return responsibility_array\n",
    "        '''\n",
    "\n",
    "\n",
    "    def Maximisation_step_M(self,responsibility_array,theta_old): #,KMEANS_cluster_array):\n",
    "        Q = self.number_of_Gaussians\n",
    "        N_q_array = np.zeros(Q)\n",
    "\n",
    "        #Calculating N_q\n",
    "        for q in range(Q):\n",
    "            for n,i in enumerate(self.DATA):\n",
    "                N_q_array[q] += responsibility_array[q][n]\n",
    "\n",
    "        #Calculating means mu\n",
    "        means_array = []\n",
    "        for q in range(Q):\n",
    "            qth_mean = 0.0\n",
    "            for n,i in enumerate(self.DATA):\n",
    "                qth_mean += (responsibility_array[q][n] * i)/N_q_array[q]\n",
    "\n",
    "            means_array.append(qth_mean)\n",
    "\n",
    "\n",
    "        #Calcuting covariance matrix array\n",
    "        covariance_matrix_array = []\n",
    "        for q in range(Q):\n",
    "            qth_cov_mat = np.zeros_like(theta_old[2][0])\n",
    "            qth_mean = means_array[q]\n",
    "            N_q = N_q_array[q]\n",
    "\n",
    "            for n,i in enumerate(self.DATA):\n",
    "                difference_from_mean = np.matrix(i - qth_mean)\n",
    "                matrix_term = np.matmul(difference_from_mean.T,difference_from_mean)\n",
    "                qth_cov_mat += (responsibility_array[q][n] * matrix_term)/N_q\n",
    "####################################\n",
    "            # covariance_matrix_array.append(qth_cov_mat)\n",
    "            if self.diag_cov_flag==1:\n",
    "                shape_cov_mat = np.shape(qth_cov_mat)\n",
    "                toappend = np.multiply(qth_cov_mat,np.eye(shape_cov_mat[0]))\n",
    "                covariance_matrix_array.append(toappend)\n",
    "\n",
    "            else:\n",
    "                covariance_matrix_array.append(qth_cov_mat)\n",
    "                 \n",
    "                \n",
    "####################################\n",
    "        #Calculating w_q\n",
    "        w_array = []\n",
    "\n",
    "        for q in range(Q):\n",
    "            N_q = N_q_array[q]\n",
    "            w_q = N_q/self.N\n",
    "            w_array.append(w_q)\n",
    "\n",
    "\n",
    "        theta_new = [w_array,means_array,covariance_matrix_array]\n",
    "        return theta_new\n",
    "\n",
    "        '''\n",
    "        #calculating Nq\n",
    "        for i in responsibility_array:\n",
    "            N_q = np.sum(i)\n",
    "            N_q_array.append(N_q)\n",
    "\n",
    "\n",
    "        #calculating means mu\n",
    "        means_array = []\n",
    "        \n",
    "        for q in range(Q):\n",
    "            N_q = N_q_array[q]\n",
    "            qth_mean = 0\n",
    "\n",
    "            for i,j in zip(responsibility_array[q],KMEANS_cluster_array[q]):\n",
    "                qth_mean += (i*j)/N_q\n",
    "\n",
    "            means_array.append(qth_mean)\n",
    "\n",
    "\n",
    "        #calculating covaricnce matrices\n",
    "        covariance_matrix_array = []\n",
    "\n",
    "        for q in range(Q):\n",
    "            N_q = N_q_array[q]\n",
    "            qth_cov_mat = np.zeros_like(theta_old[2][0])\n",
    "            qth_mean = means_array[q]\n",
    "\n",
    "            for i,j in zip(responsibility_array[q],KMEANS_cluster_array[q]):\n",
    "                difference_from_mean = np.matrix(j - qth_mean)\n",
    "                matrix_term = np.matmul(difference_from_mean.T,difference_from_mean)\n",
    "                qth_cov_mat += (i * matrix_term)/N_q\n",
    "\n",
    "            covariance_matrix_array.append(qth_cov_mat)\n",
    "\n",
    "\n",
    "        #calculating wq\n",
    "        w_array = []\n",
    "\n",
    "        for q in range(Q):\n",
    "            N_q = N_q_array[q]\n",
    "            w_q = N_q/self.N\n",
    "            w_array.append(w_q)\n",
    "\n",
    "        theta_new = [w_array,means_array,covariance_matrix_array]\n",
    "        return theta_new\n",
    "        '''\n",
    "\n",
    "    \n",
    "    def LOG_LIKELIHOOD_CALCULATOR(self,theta):\n",
    "        w_array = theta[0]\n",
    "        means_array = theta[1]\n",
    "        covariance_matrix_array = theta[2]\n",
    "\n",
    "        #CHECK SLIDES FOR LOG LIKELIHOOD FUNCTION\n",
    "        log_term = 0.0\n",
    "\n",
    "        for i in self.DATA:\n",
    "            term_inside_log = 0.0\n",
    "            for q in range(self.number_of_Gaussians):\n",
    "                term_inside_log += w_array[q] * multivariate_normal.pdf(i,means_array[q],covariance_matrix_array[q])#self.NORMAL_DISTRIBUTION(means_array[q],covariance_matrix_array[q],i)\n",
    "                #print('sdsdsf',self.NORMAL_DISTRIBUTION(means_array[q],covariance_matrix_array[q],i))   \n",
    "            #print('term_inside log',term_inside_log)\n",
    "            log_term += np.log(term_inside_log)\n",
    "\n",
    "        log_likelihood = log_term\n",
    "        return log_likelihood\n",
    "\n",
    "    \n",
    "    def likelihood_difference_step(self,theta_new,theta_old):\n",
    "        LL_OLD = self.LOG_LIKELIHOOD_CALCULATOR(theta_old)\n",
    "        LL_NEW = self.LOG_LIKELIHOOD_CALCULATOR(theta_new)\n",
    "\n",
    "        #print(LL_OLD,LL_NEW)\n",
    "\n",
    "        difference = LL_NEW - LL_OLD\n",
    "        print(difference)\n",
    "        return difference\n",
    "\n",
    "    \n",
    "\n",
    "    '''\n",
    "    def LOG_LIKELIHOOD_CALCULATOR(self, theta):\n",
    "        w_array, means_array, covariance_matrix_array = theta\n",
    "    \n",
    "        log_term = 0.0\n",
    "    \n",
    "        for i in self.DATA:\n",
    "            term_inside_log = 0.0\n",
    "            for q in range(self.number_of_Gaussians):\n",
    "                term_inside_log += w_array[q] * self.NORMAL_DISTRIBUTION(means_array[q], covariance_matrix_array[q], i)\n",
    "            log_term += np.log1p(term_inside_log)  # Use np.log1p to handle small numbers accurately\n",
    "    \n",
    "        log_likelihood = log_term\n",
    "        return log_likelihood\n",
    "\n",
    "    def likelihood_difference_step(self, theta_new, theta_old):\n",
    "        LL_OLD = self.LOG_LIKELIHOOD_CALCULATOR(theta_old)\n",
    "        LL_NEW = self.LOG_LIKELIHOOD_CALCULATOR(theta_new)\n",
    "    \n",
    "        difference = LL_NEW - LL_OLD\n",
    "        return difference\n",
    "    '''\n",
    "\n",
    "\n",
    "\n",
    "    def main_process(self,max_iterator_count):  #max_iterator_count means process will stop after that value if no convergence takes place\n",
    "        iterator = 1\n",
    "        #INITIALISATION STEP\n",
    "        theta_old = self.Intialisation_step()\n",
    "\n",
    "        while iterator <= max_iterator_count:\n",
    "            print('iterator=',iterator)\n",
    "            \n",
    "            #EXPECTATION STEP\n",
    "            responsibility_array = self.Expectation_step_E(theta_old)\n",
    "\n",
    "            #MAXIMISATION STEP\n",
    "            theta_new = self.Maximisation_step_M(responsibility_array,theta_old)\n",
    "\n",
    "            #COMPARISON STEP\n",
    "            difference = self.likelihood_difference_step(theta_new,theta_old)\n",
    "            #print(difference)\n",
    "\n",
    "            if np.abs(difference) <= self.threshold:\n",
    "             #   print('lauda lelo')\n",
    "                print('CONVERGED')\n",
    "                break\n",
    "\n",
    "            elif np.abs(difference) > self.threshold:    \n",
    "                theta_old = theta_new.copy()\n",
    "                #theta_old = self.Intialisation_step()\n",
    "\n",
    "            iterator += 1\n",
    "\n",
    "        return theta_new\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504b0a4d-aff3-42f2-bfd7-41880390aca5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### GMM-based Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e154c5e8-a47c-4590-a2bf-aba36a08cfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMM_Based_Bayes_classifier:\n",
    "    Training_data = []\n",
    "    Training_labels = []\n",
    "    label_array = []\n",
    "\n",
    "    #Test_data = []\n",
    "    #Test_labels = []\n",
    "\n",
    "    Training_data_classwise = []\n",
    "    number_of_gaussians = [] #Q\n",
    "\n",
    "    #parameters_array = []\n",
    "    N = 0\n",
    "\n",
    "    # def extract_diagonal(self,matrix):\n",
    "    #     new_matrix = np.zeros_like(matrix)\n",
    "    #     ROWS = np.shape(new_matrix)[0]\n",
    "    #     COLUMNS = np.shape(new_matrix)[1]\n",
    "    #     matrix = np.matrix(matrix)\n",
    "    #     diagonal_vector = np.array(matrix.diagonal())\n",
    "\n",
    "    #     for i in range(ROWS):\n",
    "    #         for j in range(COLUMNS):\n",
    "    #             if i==j:\n",
    "    #                 new_matrix[i][i] = diagonal_vector[i]\n",
    "\n",
    "    #     return new_matrix\n",
    "        \n",
    "    def __init__(self,TrD,TrL,Q,diagonal_cov_mat=0):  #diagonal_cov_mat flag\n",
    "        self.Training_data = TrD\n",
    "        self.Training_labels = TrL\n",
    "\n",
    "        self.label_array = np.unique(TrL)\n",
    "        \n",
    "        #self.Test_data = TeD\n",
    "        #self.Test_labels = TeL\n",
    "\n",
    "        self.number_of_gaussians = Q\n",
    "        self.N = np.shape(TrD)[0]\n",
    "\n",
    "        #Parameter estimation step\n",
    "\n",
    "        #classwise _ training data\n",
    "        n_classes = np.shape(np.unique(TrL))[0]\n",
    "\n",
    "        Training_data_classwise = []\n",
    "        for i in range(n_classes):\n",
    "            Training_data_classwise.append([])\n",
    "\n",
    "        for index,data_point in enumerate(TrD):\n",
    "            Training_data_classwise[int(TrL[index])].append(data_point)\n",
    "\n",
    "        self.Training_data_classwise = Training_data_classwise\n",
    "\n",
    "        #Parameter_estimation_step\n",
    "        parameter_array = []\n",
    "        for i in range(n_classes):\n",
    "            parameter_array.append([])\n",
    "\n",
    "        for i in range(n_classes):\n",
    "            PARAM_EST_OBJECT = Expectation_Maximisation_FOR_GMM_Params(self.Training_data_classwise[i],self.number_of_gaussians,0.001,diagonal_cov_mat)\n",
    "            theta = PARAM_EST_OBJECT.main_process(110)  #param is number of steps before process stops\n",
    "            \n",
    "            '''\n",
    "            if diagonal_cov_mat==1:\n",
    "                for j in range(Q):\n",
    "                    #theta[j] is a covariance matrix\n",
    "                    theta[j][2] = np.multiply(theta[j][2],np.eye(np.shape(theta[j][2])[0]))\n",
    "            '''\n",
    "            \n",
    "\n",
    "            parameter_array[i] = theta\n",
    "\n",
    "        \n",
    "        self.parameters_array = parameter_array\n",
    "        #print(parameter_array)\n",
    "\n",
    "    \n",
    "    def NORMAL_DISTRIBUTION(self,mu,cov_mat,Point):\n",
    "        inv_cov_mat = np.linalg.inv(cov_mat)\n",
    "        difference_from_mean = np.matrix(Point - mu)\n",
    "        d = np.shape(Point)[0]\n",
    "        #print(d)\n",
    "\n",
    "        exponentail_term_power = -(np.matmul(np.matmul(difference_from_mean,inv_cov_mat),difference_from_mean.T))/2\n",
    "\n",
    "        rational_term_denominator = np.sqrt(((2*np.pi)**d) * np.linalg.det(cov_mat))\n",
    "        #print(rational_term_denominator)\n",
    "\n",
    "        Normal_prob = np.exp(exponentail_term_power[0,0])/(rational_term_denominator) #regulariation\n",
    "        return Normal_prob\n",
    "    \n",
    "    \n",
    "    def LIKELIHOOD_CALCULATOR(self,theta,point):\n",
    "        w_array = theta[0]\n",
    "        means_array = theta[1]\n",
    "        cov_mat_array = theta[2]\n",
    "\n",
    "        LIKELIHOOD = 0.0\n",
    "\n",
    "        for i in range(self.number_of_gaussians): #number_of_gaussians = Q\n",
    "            Prob_from_gaussian_at_i = w_array[i]* multivariate_normal.pdf(point,means_array[i],cov_mat_array[i])\n",
    "            LIKELIHOOD += Prob_from_gaussian_at_i\n",
    "\n",
    "        return LIKELIHOOD\n",
    "            \n",
    "\n",
    "        \n",
    "        \n",
    "    def Posterioir_Probability_calculator(self,test_point):\n",
    "        n_classes =  np.shape(np.unique(self.Training_labels))[0]\n",
    "        Posterioir_prob = np.zeros(n_classes)\n",
    "\n",
    "        #Now doing fpr each class\n",
    "        for i in range(n_classes):\n",
    "            P_y_i = (np.shape(self.Training_data_classwise[i])[0])/self.N #Prior Probability\n",
    "\n",
    "            #Calculating likelihood\n",
    "            theta = self.parameters_array[i]\n",
    "\n",
    "            LIKELIHOOD = self.LIKELIHOOD_CALCULATOR(theta,test_point)#self.LIKELIHOOD_CALCULATOR(theta,test_point)\n",
    "\n",
    "            POSTERIOIR_PROB_i = LIKELIHOOD*P_y_i\n",
    "            Posterioir_prob[i] = POSTERIOIR_PROB_i\n",
    "\n",
    "        return Posterioir_prob\n",
    "\n",
    "    def single_point_classifier(self,test_point):\n",
    "        Posterioir_prob_array = self.Posterioir_Probability_calculator(test_point)\n",
    "        max_posterioir_prob_index = np.argmax(Posterioir_prob_array)\n",
    "\n",
    "        #print(Posterioir_prob_array)\n",
    "\n",
    "        classified_label = self.label_array[max_posterioir_prob_index]\n",
    "        return classified_label\n",
    "\n",
    "    def classifier_on_test_data(self,TEST_DATA):\n",
    "        classfication_labels = []\n",
    "    \n",
    "        index = 0 #############3\n",
    "        for i in TEST_DATA:\n",
    "            #print(index) ##############33\n",
    "            classified_label = self.single_point_classifier(i)\n",
    "            classfication_labels.append(classified_label)\n",
    "            index +=1\n",
    "    \n",
    "        return classfication_labels\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44040f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "row = 0\n",
    "column = 0\n",
    "K = 0\n",
    "\n",
    "def initialize():\n",
    "\n",
    "    global mean, weight, deviation\n",
    "    mean = np.zeros(shape=(K, row, column, 3))\n",
    "    weight = np.ones(shape=(K, row, column)) / K\n",
    "    deviation = np.ones(shape=(K, row, column))\n",
    "    \n",
    "def check():\n",
    "\n",
    "    global mask_divide, idx\n",
    "    T = 0.7\n",
    "    ratio = -1*(weight / deviation)\n",
    "    idx = ratio.argsort(axis=0)\n",
    "    ratio.sort(axis=0)\n",
    "    ratio *= -1\n",
    "    cum = np.cumsum(ratio, axis=0)\n",
    "    mask_divide = (cum < T)\n",
    "    mask_divide = np.choose(idx, mask_divide)\n",
    "    # mask_divide = np.choose(idx, mask_divide)\n",
    "\n",
    "def mahalanobis_probability(video):\n",
    "\n",
    "    global mask_distance, prob\n",
    "\n",
    "    temp = np.subtract(video, mean)\n",
    "    temp = np.sum(temp**2, axis=3) / (deviation**2)\n",
    "\n",
    "    prob = np.exp(temp/(-2)) / (np.sqrt((2*np.pi)**3)*deviation)\n",
    "\n",
    "    temp = np.sqrt(temp)\n",
    "    mask_distance = (temp < 2.5*deviation)\n",
    "\n",
    "def update(video):\n",
    "    \n",
    "    global weight, mask_distance, mean, deviation, prob, mask_some\n",
    "\n",
    "    alpha = 0.2\n",
    "    rho = alpha * prob\n",
    "    \n",
    "    mask_some = np.bitwise_or.reduce(mask_distance, axis=0)\n",
    "    mask_update = np.where(mask_some == True, mask_distance, -1)\n",
    "\n",
    "    weight = np.where(mask_update == 1, (1 - alpha) * weight + alpha, weight)\n",
    "    weight = np.where(mask_update == 0, (1 - alpha) * weight, weight)\n",
    "    #weight = np.where(mask_update == -1, 0.0001, weight)\n",
    "\n",
    "    data = np.stack([video]*K, axis=0)\n",
    "    mask = np.stack([mask_update]*3, axis=3)\n",
    "    r = np.stack([rho]*3, axis=3)\n",
    "    \n",
    "    mean = np.where(mask == 1, (1 - r) * mean + r * data, mean)\n",
    "    mean = np.where(mask == -1, data, mean)\n",
    "    \n",
    "    deviation = np.where(mask_update == 1, np.sqrt((1-rho)*(deviation**2) + rho*(np.sum(np.subtract(video, mean)**2, axis=3))), deviation)\n",
    "    deviation = np.where(mask_update == -1, 3 + np.ones(shape=(K, row, column)), deviation)\n",
    "\n",
    "\n",
    "def result(video):\n",
    "\n",
    "    background = np.zeros(shape=(row, column, 3), dtype=np.uint8)\n",
    "    foreground = 255 + np.zeros(shape=(row, column, 3), dtype=np.uint8)\n",
    "    m = np.stack([mask_some]*3, axis=2)\n",
    "    res = np.where(m == False, foreground, background)\n",
    "\n",
    "    n = np.bitwise_and(mask_divide, mask_distance)\n",
    "    n = np.bitwise_or.reduce(n, axis=0)\n",
    "    n = np.stack([n]*3, axis=2)\n",
    "    res = np.where(n == True, background, foreground)\n",
    "\n",
    "    # cv2.imshow('frame', video)\n",
    "    cv2.imshow('res', res)\n",
    "\n",
    "def frame_processing(video):\n",
    "    \n",
    "    check()\n",
    "    mahalanobis_probability(video)\n",
    "    update(video)\n",
    "    result(video)\n",
    "\n",
    "def main():\n",
    "\n",
    "    global row, column, K\n",
    "    cap = cv2.VideoCapture('vtest.avi')\n",
    "    #cap = cv2.VideoCapture(0)\n",
    "    ret, frame = cap.read()\n",
    "    K = 3\n",
    "    row, column, _ = frame.shape\n",
    "    initialize()\n",
    "    fgbg = cv2.createBackgroundSubtractorMOG2(False)\n",
    "    while(1):\n",
    "        ret, frame = cap.read()\n",
    "        #frame = 128 + np.zeros(shape=(row, column, 3), dtype=np.uint8)\n",
    "        frame_processing(frame)\n",
    "        cv2.imshow('frame', frame)\n",
    "        fgmask = fgbg.apply(frame)\n",
    "        cv2.imshow('fgmask', fgmask)\n",
    "        k = cv2.waitKey(30) & 0xff\n",
    "        if k == 27:\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
